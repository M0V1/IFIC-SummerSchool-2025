{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#install required packages\n",
        "import sys\n",
        "!pip install atlasopenmagic\n",
        "from atlasopenmagic import install_from_environment\n",
        "install_from_environment()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThSibgQqkiJs",
        "outputId": "4c7bece2-f3c8-4c20-8d34-88db7cf01478"
      },
      "id": "ThSibgQqkiJs",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: atlasopenmagic in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from atlasopenmagic) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from atlasopenmagic) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->atlasopenmagic) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->atlasopenmagic) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->atlasopenmagic) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->atlasopenmagic) (2025.7.9)\n",
            "Installing packages: ['aiohttp>=3.9.5', 'atlasopenmagic>=1.0.1', 'awkward>=2.6.7', 'awkward-pandas>=2023.8.0', 'coffea~=0.7.0', 'hist>=2.8.0', 'ipykernel>=6.29.5', 'jupyter>=1.0.0', 'lmfit>=1.3.2', 'matplotlib>=3.9.1', 'metakernel>=0.30.2', 'notebook<7', 'numpy>=1.26.4', 'pandas>=2.2.2', 'papermill>=2.6.0', 'pip>=24.2', 'scikit-learn>=1.5.1', 'uproot>=5.3.10', 'uproot3>=3.14.4', 'fsspec-xrootd>=0.5.1', 'jupyterlab_latex~=3.1.0', 'vector>=1.4.1']\n",
            "Installation complete. You may need to restart your Python environment for changes to take effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import uproot # For reading ROOT files efficiently\n",
        "import awkward as ak # To represent nested data in columnar format\n",
        "import pandas as pd # For dataframes, a format widely used in python\n",
        "import numpy as np # For numerical calculations such as histogramming\n",
        "import time # For timing operations and adding delays if needed\n",
        "import matplotlib.pyplot as plt # For creating plots and visualizations\n",
        "from matplotlib.ticker import AutoMinorLocator # for minor ticks\n",
        "import atlasopenmagic as atom  # Provides access to ATLAS Open Data metadata and streaming URLs\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed # Enables parallel execution for faster processing of large datasets\n",
        "# Filter warnings that otherwise appear in output. These are normal in the running of this notebook.\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in sqrt\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in power\")\n",
        "warnings.filterwarnings(\"ignore\", message=\"overflow encountered in multiply\", category=RuntimeWarning)\n",
        "warnings.filterwarnings(\"ignore\", message=\"invalid value encountered in subtract\")\n"
      ],
      "metadata": {
        "id": "2m6q7qfbkjvc"
      },
      "id": "2m6q7qfbkjvc",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Integrated luminosity in inverse picobarns\n",
        "lumi = 36000.\n",
        "\n",
        "# Fraction of events to process\n",
        "fraction = 0.25"
      ],
      "metadata": {
        "id": "dAmg9Ra2kjyF"
      },
      "id": "dAmg9Ra2kjyF",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "atom.set_release('2025e-13tev-beta')\n",
        "\n",
        "mc_defs = {\n",
        "    r'ttbar_Ztt':    {'dids': [410470]}, #,700901, 700902],},\n",
        "    r'tautau': {'dids': [346919] },#, 346923, 346927]},\n",
        "    r'Higgs':  {'dids': [345120] },#, 345121, 345122, 345123]},\n",
        "}\n",
        "\n",
        "mc_samples   = atom.build_mc_dataset(mc_defs,   skim='2J2LMET30', protocol='https')\n",
        "data_samples_1 = atom.build_data_dataset('2J2LMET30', name=\"Data\", protocol='https')\n",
        "\n",
        "samples = {**data_samples_1, **mc_samples}\n",
        "\n",
        "variables = [\"mcWeight\", \"ScaleFactor_DiTauTRIGGER\", \"trigE\", \"trigDT\", \"lep_n\", \"lep_pt\",\"ScaleFactor_BTAG\",\"lep_isMediumID\",\n",
        "            \"lep_eta\", \"lep_phi\", \"lep_charge\", \"lep_type\",\"tau_n\",\"tau_pt\", \"tau_e\", \"tau_eta\", \"tau_phi\", \"met\", \"met_phi\", \"sum_of_weights\", \"ScaleFactor_FTAG\",\"lep_isLooseIso\",\n",
        "            \"xsec\", \"jet_pt\", \"jet_btag_quantile\", \"jet_n\", \"jet_eta\", \"jet_phi\", \"jet_e\",\"jet_jvt\",\"lep_isTrigMatched\",\n",
        "            \"lep_type\", \"lep_e\", \"eventNumber\", \"ScaleFactor_ELE\", \"ScaleFactor_TAU\",\"ScaleFactor_PILEUP\",\"filteff\", \"kfac\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3eEHG71rkj0d",
        "outputId": "edd26499-6352-448e-b64d-e53f4acf248b"
      },
      "id": "3eEHG71rkj0d",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Active release set to: 2025e-13tev-beta. Metadata cache cleared.\n",
            "\u001b[91mDeprecationWarning: The build_mc_dataset function is deprecated. Use build_dataset with the appropriate MC definitions instead. (/usr/local/lib/python3.11/dist-packages/atlasopenmagic/utils.py:173)\u001b[0m\n",
            "Fetching and caching all metadata for release: 2025e-13tev-beta...\n",
            "Successfully cached 374 datasets.\n",
            "\u001b[91mDeprecationWarning: The build_data_dataset function is deprecated. Use build_dataset with the appropriate data definitions instead. (/usr/local/lib/python3.11/dist-packages/atlasopenmagic/utils.py:161)\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_weight(data):\n",
        "    weight_list =( data[\"ScaleFactor_TAU\"] * data[\"ScaleFactor_DiTauTRIGGER\"] * data[\"ScaleFactor_PILEUP\"] *\n",
        "             ( data[\"ScaleFactor_BTAG\"] * data[\"mcWeight\"] / data[\"sum_of_weights\"]) * (data[\"xsec\"] * data[\"filteff\"] * data[\"kfac\"] * lumi) )\n",
        "    return weight_list"
      ],
      "metadata": {
        "id": "ycR2vPNrkj25"
      },
      "id": "ycR2vPNrkj25",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_trig(trigDT):\n",
        "    return trigDT"
      ],
      "metadata": {
        "id": "MHHe1Ehvkj5H"
      },
      "id": "MHHe1Ehvkj5H",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def two_lep(tau_n):\n",
        "    return tau_n == 2"
      ],
      "metadata": {
        "id": "AbfrqUuqkj7e"
      },
      "id": "AbfrqUuqkj7e",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_lep_pt(tau_pt):\n",
        "    return ak.sum(tau_pt > 20, axis=1) >= 2"
      ],
      "metadata": {
        "id": "z4bKcD1dnHd7"
      },
      "id": "z4bKcD1dnHd7",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_met_et(met_et):\n",
        "    return met_et > 20"
      ],
      "metadata": {
        "id": "Fj6WA9HJnHgj"
      },
      "id": "Fj6WA9HJnHgj",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_charge(charge):\n",
        "    return ak.sum(charge, axis=1) == 0"
      ],
      "metadata": {
        "id": "bBtvLn05nHi0"
      },
      "id": "bBtvLn05nHi0",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_type(type1):\n",
        "    return ak.sum(type1 == 15, axis=1) == 2"
      ],
      "metadata": {
        "id": "7xFgB3TNnHlF"
      },
      "id": "7xFgB3TNnHlF",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_jet(E,pt,b_tag):\n",
        "  E_check = ak.sum(E >= 30 , axis=1) >= 2\n",
        "  pt_check = ak.sum(pt >= 30, axis=1) >= 2\n",
        "  btag_check = ak.sum(b_tag>=3, axis=1) < 1\n",
        "  return (pt_check) & (E_check) & (btag_check)"
      ],
      "metadata": {
        "id": "6KrWCiO8nHnV"
      },
      "id": "6KrWCiO8nHnV",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_DR(eta,phi,eta_M,phi_M):\n",
        "\n",
        "    Dphi1 = np.arccos( np.cos(phi -  phi_M[:,0]) )\n",
        "    Dphi2 = np.arccos( np.cos(phi -  phi_M[:,1]) )\n",
        "\n",
        "    Deta1 = eta - eta_M[:,0]\n",
        "    Deta2 = eta - eta_M[:,1]\n",
        "\n",
        "    DR_M1 = np.sqrt((Deta1)**2 + (Dphi1)**2)\n",
        "    DR_M2 = np.sqrt((Deta2)**2 + (Dphi2)**2)\n",
        "\n",
        "    DR_M1_check = ak.sum(DR_M1 >= 0.4, axis=1) >= 1\n",
        "    DR_M2_check = ak.sum(DR_M2 >= 0.4, axis=1) >= 1\n",
        "\n",
        "    return (DR_M1_check) & (DR_M2_check)"
      ],
      "metadata": {
        "id": "iVvH41CQnHp4"
      },
      "id": "iVvH41CQnHp4",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_VBF(jet_E,pt,eta,phi):\n",
        "    jet_px = pt * np.cos(phi)\n",
        "    jet_py = pt * np.sin(phi)\n",
        "    jet_pz = pt / np.tan(2.0 * np.arctan( np.exp( -eta ) ) )\n",
        "\n",
        "    # Helper function to create combinations of jets\n",
        "    def combo(list_1):\n",
        "        jets_pairs = ak.combinations(list_1, 2, fields=['List1', 'List2'])\n",
        "        sum_List = jets_pairs['List1'] + jets_pairs['List2']\n",
        "        return sum_List\n",
        "\n",
        "    combo_jet_E = combo(jet_E)\n",
        "    combo_jet_px = combo(jet_px)\n",
        "    combo_jet_py = combo(jet_py)\n",
        "    combo_jet_pz = combo(jet_pz)\n",
        "\n",
        "    Mass = np.sqrt(combo_jet_E**2 -(combo_jet_px**2 + combo_jet_py**2 + combo_jet_pz**2))\n",
        "\n",
        "    jets_pairs = ak.combinations(eta, 2, fields=['List1', 'List2'])\n",
        "    abs_dif_eta = np.abs(jets_pairs['List1'] - jets_pairs['List2'])\n",
        "    eta_mult = jets_pairs['List1'] * jets_pairs['List2']\n",
        "\n",
        "    return ak.sum((abs_dif_eta > 3) & (eta_mult < 0) & (Mass>=500), axis=1) > 0"
      ],
      "metadata": {
        "id": "tZJ4tldsnHsU"
      },
      "id": "tZJ4tldsnHsU",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Hmass(E,pt,eta,phi):\n",
        "    tau_E = ak.sum(E, axis=1)\n",
        "    tau_px = ak.sum(pt * np.cos(phi), axis=1)\n",
        "    tau_py = ak.sum(pt * np.sin(phi), axis=1)\n",
        "    tau_pz = ak.sum(pt / np.tan(2.0 * np.arctan( np.exp( -eta ) ) ), axis=1)\n",
        "    Mass = np.sqrt(tau_E**2 -(tau_px**2 + tau_py**2 + tau_pz**2))\n",
        "    return Mass"
      ],
      "metadata": {
        "id": "eBY7eM8qnHuw"
      },
      "id": "eBY7eM8qnHuw",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(file_path, sample_name,loop):\n",
        "    # Open the 'analysis' TTree from the ROOT file\n",
        "    tree = uproot.open(file_path + \":analysis\")\n",
        "    sample_data = []\n",
        "\n",
        "    for data in tree.iterate(variables, library=\"ak\", entry_start=int(tree.num_entries * fraction * loop),\n",
        "                             entry_stop=int(tree.num_entries * fraction * (loop+1))):\n",
        "\n",
        "            #data = data[cut_trig(data.trigDT)]\n",
        "            #data = data[Matched_cut(data.lep_isTrigMatched)]\n",
        "            #data = data[two_lep(data.tau_n)]\n",
        "           # data = data[cut_met_et(data.met)]\n",
        "           # data = data[cut_type(data.tau_type)]\n",
        "           # data = data[cut_lep_pt(data.tau_pt)]\n",
        "           # data = data[cut_charge(data.tau_charge)]\n",
        "            #data = data[ID_iso_cut(data.lep_isMediumID, data.lep_isLooseIso)]\n",
        "           # data = data[cut_jet(data.jet_e, data.jet_pt, data.jet_btag_quantile)]\n",
        "           # data = data[cut_DR(data.jet_eta, data.jet_phi, data.tau_eta, data.tau_phi)]\n",
        "           # data = data[cut_VBF(data.jet_e, data.jet_pt, data.jet_eta, data.jet_phi)]\n",
        "\n",
        "\n",
        "            data['Inv_mass'] = Hmass(data.tau_e, data.tau_pt, data.tau_eta, data.tau_phi)\n",
        "\n",
        "            if 'data' not in sample_name:\n",
        "                data['Weight'] = calc_weight(data)\n",
        "            #else:\n",
        "              #  data['Weight'] = ak.ones_like(data['met'])\n",
        "\n",
        "            sample_data.append(data)\n",
        "\n",
        "    # Concatenate all data from the current file into a single array\n",
        "    return ak.concatenate(sample_data, axis=0)"
      ],
      "metadata": {
        "id": "ZJD9Xcl6oPHd"
      },
      "id": "ZJD9Xcl6oPHd",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parallel_analysis(file_path, sample_name):\n",
        "    # Parallel processing\n",
        "    with ProcessPoolExecutor() as executor:\n",
        "        # Submit all tasks using a dictionary comprehension\n",
        "        futures = {\n",
        "          executor.submit(process_file, file_path, sample_name, i): i\n",
        "          for i in range(10)\n",
        "        }\n",
        "\n",
        "        results = []\n",
        "        for future in as_completed(futures):\n",
        "            try:\n",
        "                results.append(future.result())\n",
        "            except Exception as e:\n",
        "                print(f\"Error in {sample_name} loop {futures[future]}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Combine results into an Awkward Array\n",
        "    combined_array = ak.concatenate(results, axis=0) if results else ak.Array([])\n",
        "\n",
        "    return combined_array\n"
      ],
      "metadata": {
        "id": "ol_rESUnoPJ7"
      },
      "id": "ol_rESUnoPJ7",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_all = time.time()  # Define start time\n",
        "data_all = {}  # Dictionary to store results for each sample\n",
        "fraction = 0.2 # Lower this vaule for less running time\n",
        "print(\"The analysis has started\")\n",
        "\n",
        "for s in samples:\n",
        "    frames = []\n",
        "    print(\"processing the \",s,\" samples\")\n",
        "\n",
        "    # Loop over ROOT files associated with the current sample\n",
        "    for val in samples[s]['list']:\n",
        "\n",
        "        DF = parallel_analysis(val, s)\n",
        "        frames.append(DF) # Collect the results\n",
        "\n",
        "    # Store the frames for this sample\n",
        "    data_all[s] = ak.concatenate(frames, axis=0)\n",
        "\n",
        "end_all = time.time()\n",
        "print(f\"\\nTotal time taken to process all samples: {round((end_all - start_all) / 60, 1)} minutes\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zijVo512oPMW",
        "outputId": "86ee42ef-661a-4415-b037-0dab6482e3e0"
      },
      "id": "zijVo512oPMW",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The analysis has started\n",
            "processing the  Data  samples\n",
            "Error in Data loop 5: reduce() of empty iterable with no initial value\n",
            "Error in Data loop 7: reduce() of empty iterable with no initial value\n",
            "Error in Data loop 6: reduce() of empty iterable with no initial value\n",
            "Error in Data loop 8: reduce() of empty iterable with no initial value\n",
            "Error in Data loop 9: reduce() of empty iterable with no initial value\n",
            "Error in Data loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 4: 'ClientResponseError' object is not subscriptable\n",
            "Error in Data loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Data loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "processing the  ttbar_Ztt  samples\n",
            "Error in ttbar_Ztt loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in ttbar_Ztt loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in ttbar_Ztt loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in ttbar_Ztt loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in ttbar_Ztt loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in ttbar_Ztt loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in ttbar_Ztt loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in ttbar_Ztt loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in ttbar_Ztt loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in ttbar_Ztt loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "processing the  tautau  samples\n",
            "Error in tautau loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in tautau loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in tautau loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in tautau loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in tautau loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in tautau loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in tautau loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in tautau loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in tautau loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in tautau loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "processing the  Higgs  samples\n",
            "Error in Higgs loop 1: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Higgs loop 0: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Higgs loop 2: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Higgs loop 3: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Higgs loop 4: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Higgs loop 5: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Higgs loop 6: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Higgs loop 7: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Higgs loop 8: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "Error in Higgs loop 9: can't pickle multidict._multidict.CIMultiDictProxy objects\n",
            "\n",
            "Total time taken to process all samples: 0.9 minutes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_data(data,fit):\n",
        "\n",
        "    # Define plot parameters\n",
        "    xmin, xmax, step_size = 110, 160, 2\n",
        "\n",
        "    # Define MC data sets and their properties\n",
        "    datasets = [\n",
        "        {'data': data['ttbar_Ztt']['Inv_mass'], 'weights': data['ttbar_Ztt']['Weight'], 'color': 'cyan', 'label': r''},\n",
        "        {'data': data['tautau']['Inv_mass'], 'weights': data['tautau']['Weight'], 'color': 'orange', 'label': r''}]\n",
        "\n",
        "    # Create bin edges and centers\n",
        "    bin_edges = np.arange(xmin, xmax + step_size, step_size)\n",
        "    bin_centres = np.arange(xmin + step_size/2, xmax + step_size/2, step_size)\n",
        "\n",
        "    # Compute the histogram of the data\n",
        "    data_x, _ = np.histogram(data['Data']['Inv_mass'], bins=bin_edges)\n",
        "\n",
        "    data_x_errors = np.sqrt(data_x)  # statistical error on the data\n",
        "\n",
        "    # Create main plot and residual subplot\n",
        "    fig, (main_axes, residual_axes) = plt.subplots(2, 1, figsize=(7, 6), gridspec_kw={'height_ratios': [3, 1]}, sharex=True)\n",
        "\n",
        "    # Plot data with error bars\n",
        "    Cut1 = (bin_centres >= xmin) & (bin_centres <= xmax)  # Cut for main plotting range\n",
        "    main_axes.errorbar(x=bin_centres, y=data_x, yerr=data_x_errors, fmt='ko', label=f'Data entries = {sum(data_x[Cut1])}')\n",
        "\n",
        "    # Plot the Monte Carlo bars\n",
        "    mc_heights = main_axes.hist([d['data'] for d in datasets], bins=bin_edges, weights=[d['weights'] for d in datasets],\n",
        "                                stacked=True,color=[d['color'] for d in datasets], label=[d['label'] for d in datasets])\n",
        "\n",
        "    mc_x_tot = (mc_heights[0][1])  # Stacked background MC y-axis value\n",
        "\n",
        "    # Calculate MC statistical uncertainty: sqrt(sum w^2)\n",
        "    mc_x_err = np.sqrt(np.histogram(np.hstack([d['data'] for d in datasets]), bins=bin_edges,weights=np.hstack([d['weights'] for d in datasets])**2)[0])\n",
        "\n",
        "    # Plot the statistical uncertainty\n",
        "    main_axes.bar(bin_centres, 2*mc_x_err, alpha=0.5, bottom=mc_x_tot-mc_x_err,color='none', hatch=\"////\", width=step_size, label='Stat. Unc.')\n",
        "\n",
        "    # Set up main axes\n",
        "    main_axes.set_xlim(left=xmin, right=xmax)\n",
        "\n",
        "    if fit == True:\n",
        "        higgs_hist, _ = np.histogram(data['Higgs']['Inv_mass'], bins=bin_edges, weights=data['Higgs']['Weight']*100)\n",
        "        main_axes.step(bin_centres, higgs_hist, where='mid', color='purple', linewidth=3, label='Higgs *100')\n",
        "\n",
        "    # Add headspace to the plot\n",
        "    ymax = max(np.max(data_x), np.max(np.sum(mc_heights[0], axis=0)))\n",
        "    main_axes.set_ylim(0, ymax * 1.4)  # Add 40% headspace\n",
        "    main_axes.xaxis.set_minor_locator(AutoMinorLocator())\n",
        "    main_axes.tick_params(which='both', direction='in', top=True, right=True)\n",
        "    main_axes.set_ylabel('Events', y=1, horizontalalignment='right')\n",
        "    main_axes.yaxis.set_minor_locator(AutoMinorLocator())\n",
        "\n",
        "    # Add text to the plot\n",
        "    main_axes.text(0.05, 0.93, 'ATLAS Open Data', transform=main_axes.transAxes, fontsize=13)\n",
        "    main_axes.text(0.05, 0.88, 'for education', transform=main_axes.transAxes, style='italic', fontsize=8)\n",
        "    main_axes.text(0.05, 0.82, r'=13 TeV, 36 fb', transform=main_axes.transAxes)\n",
        "\n",
        "    main_axes.legend(frameon=False)\n",
        "    # Calculate and plot residuals\n",
        "    ratio = data_x / np.sum(mc_heights[0], axis=0)\n",
        "    residual_axes.errorbar(bin_centres, ratio, yerr=abs(ratio*data_x_errors/data_x), fmt='ko')\n",
        "    residual_axes.axhline(1, color='r', linestyle='--')\n",
        "    residual_axes.set_xlabel(r\"tautau\", fontsize=13, x=1, horizontalalignment='right')\n",
        "    residual_axes.set_ylabel('Ratio (Data/MC)')\n",
        "    residual_axes.xaxis.set_minor_locator(AutoMinorLocator())\n",
        "    residual_axes.yaxis.set_minor_locator(AutoMinorLocator())\n",
        "    residual_axes.tick_params(which='both', direction='in', top=True, right=True)\n",
        "    residual_axes.set_ylim(top=3, bottom=0.5)\n",
        "\n",
        "    # Adjust layout\n",
        "    fig.tight_layout()\n",
        "    fig.subplots_adjust(hspace=0.05)\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "p11UMFH8oPOr"
      },
      "id": "p11UMFH8oPOr",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Contentdata_all:\", data_all)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ou-0cm3nqxuh",
        "outputId": "313961f5-5e24-4767-e258-c58b54025f12"
      },
      "id": "Ou-0cm3nqxuh",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contentdata_all: {'Data': <Array [{mcWeight: 1, ...}, {...}, ..., {...}] type='73473 * {mcWeight: flo...'>, 'ttbar_Ztt': <Array [] type='0 * unknown'>, 'tautau': <Array [] type='0 * unknown'>, 'Higgs': <Array [] type='0 * unknown'>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_data(data_all,True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "uIWwSxKJo365",
        "outputId": "d2f2513a-cacf-49e6-8595-9ca982ca846b"
      },
      "id": "uIWwSxKJo365",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "cannot slice EmptyArray (of length 0) with 'Inv_mass': not an array of records",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-21-506548338.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_all\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-19-242365927.py\u001b[0m in \u001b[0;36mplot_data\u001b[0;34m(data, fit)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Define MC data sets and their properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     datasets = [\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;34m{\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ttbar_Ztt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Inv_mass'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'weights'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ttbar_Ztt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Weight'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'color'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'cyan'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34mr''\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         {'data': data['tautau']['Inv_mass'], 'weights': data['tautau']['Weight'], 'color': 'orange', 'label': r''}]\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/awkward/highlevel.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m   1102\u001b[0m         \u001b[0mhave\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mdimension\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marray\u001b[0m \u001b[0mbeing\u001b[0m \u001b[0mindexed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1103\u001b[0m         \"\"\"\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSlicingErrorContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;31m# Handle named axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminmax_depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/awkward/_errors.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exception_type, exception_value, traceback)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# Handle caught exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorate_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexception_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m# Step out of the way so that another ErrorContext can become primary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/awkward/highlevel.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, where)\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mNamedAxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnamed_axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m             \u001b[0mindexed_layout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNamedAxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mNamedAxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/awkward/contents/content.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, where, named_axis)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_field\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.11/site-packages/awkward/contents/emptyarray.py\u001b[0m in \u001b[0;36m_getitem_field\u001b[0;34m(self, where, only_fields)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mSupportsIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_fields\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     ) -> Content:\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mak\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_errors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"not an array of records\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     def _getitem_fields(\n",
            "\u001b[0;31mIndexError\u001b[0m: cannot slice EmptyArray (of length 0) with 'Inv_mass': not an array of records"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d6026ad-09d5-4c53-97fd-cabdd3646084",
      "metadata": {
        "id": "8d6026ad-09d5-4c53-97fd-cabdd3646084"
      },
      "outputs": [],
      "source": [
        "!rm -rf IFIC-SummerSchool-2025\n",
        "!git clone https://github.com/M0V1/IFIC-SummerSchool-2025.git\n",
        "!pip install  numpy pandas uproot matplotlib mplhep awkward"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls IFIC-SummerSchool-2025\n",
        "!find IFIC-SummerSchool-2025 -name \"*.root\""
      ],
      "metadata": {
        "id": "7CQWQWwMPtvJ"
      },
      "id": "7CQWQWwMPtvJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import uproot\n",
        "import awkward as ak\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import AutoMinorLocator\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import time\n",
        "\n",
        "# Integrated luminosity in inverse picobarns\n",
        "lumi = 36000."
      ],
      "metadata": {
        "id": "5a-gEr-IFwEh"
      },
      "id": "5a-gEr-IFwEh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "files = [\n",
        "    \"IFIC-SummerSchool-2025/ODEO_FEB2025_v0_2J2LMET30_mc_700903.Sh_2214_Ztt_maxHTpTV2_Mll10_40_CVetoBVeto.2J2LMET30.root\",   #bkg\n",
        "    \"IFIC-SummerSchool-2025/ODEO_FEB2025_v0_2J2LMET30_mc_410471.PhPy8EG_A14_ttbar_hdamp258p75_allhad.2J2LMET30.root\",     #bkg\n",
        "    \"IFIC-SummerSchool-2025/ODEO_FEB2025_v0_2J2LMET30_mc_345121.PowhegPy8EG_NNLOPS_nnlo_30_ggH125_tautaulm15hp20.2J2LMET30.root\", #signal\n",
        "    \"IFIC-SummerSchool-2025/ODEO_FEB2025_v0_2J2LMET30_mc_345122.PowhegPy8EG_NNLOPS_nnlo_30_ggH125_tautaulp15hm20.2J2LMET30.root\",#signal\n",
        "    \"IFIC-SummerSchool-2025/ODEO_FEB2025_v0_2J2LMET30_mc_346343.PhPy8EG_A14NNPDF23_NNPDF30ME_ttH125_allhad.2J2LMET30.root\",\n",
        "    \"IFIC-SummerSchool-2025/ODEO_FEB2025_v0_2J2LMET30_mc_700792.Sh_2214_Ztautau_maxHTpTV2_BFilter.2J2LMET30.root\",     #bkg\n",
        "    \"IFIC-SummerSchool-2025/ODEO_FEB2025_v0_2J2LMET30_data15_periodF.2J2LMET30.root\",  #data\n",
        "    \"IFIC-SummerSchool-2025/ODEO_FEB2025_v0_2J2LMET30_mc_346344.PhPy8EG_A14NNPDF23_NNPDF30ME_ttH125_semilep.2J2LMET30.root\", #signal\n",
        "    \"IFIC-SummerSchool-2025/ODEO_FEB2025_v0_2J2LMET30_data15_periodD.2J2LMET30.root\",  #data\n",
        "    \"IFIC-SummerSchool-2025/ODEO_FEB2025_v0_2J2LMET30_data15_periodH.2J2LMET30.root\",  #data\n",
        "    \"IFIC-SummerSchool-2025/ODEO_FEB2025_v0_2J2LMET30_mc_700902.Sh_2214_Ztt_maxHTpTV2_Mll10_40_CFilterBVeto.2J2LMET30.root\", #bkg\n",
        "    \"IFIC-SummerSchool-2025/ODEO_FEB2025_v0_2J2LMET30_mc_345120.PowhegPy8EG_NNLOPS_nnlo_30_ggH125_tautaul13l7.2J2LMET30.root\",\n",
        "    \"IFIC-SummerSchool-2025/ODEO_FEB2025_v0_2J2LMET30_mc_700901.Sh_2214_Ztt_maxHTpTV2_Mll10_40_BFilter.2J2LMET30.root\" #bkg\n",
        "]\n",
        "\n",
        "tree_name = \"analysis\"\n"
      ],
      "metadata": {
        "id": "voT6Zuo3FwHp"
      },
      "id": "voT6Zuo3FwHp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_weight(data):\n",
        "    weight_list = (data[\"ScaleFactor_TAU\"] * data[\"ScaleFactor_DiTauTRIGGER\"] * data[\"ScaleFactor_PILEUP\"] *\n",
        "                   (data[\"ScaleFactor_BTAG\"] * data[\"mcWeight\"] / data[\"sum_of_weights\"]) *\n",
        "                   (data[\"xsec\"] * data[\"filteff\"] * data[\"kfac\"] * lumi))\n",
        "    return weight_list"
      ],
      "metadata": {
        "id": "ArchEXfgFwKk"
      },
      "id": "ArchEXfgFwKk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_trig(trigDT):\n",
        "    return trigDT\n",
        "\n",
        "def cut_met_et(met_et):\n",
        "    return met_et > 20\n",
        "\n",
        "def cut_leading_jet(jet_pt):\n",
        "    return ak.sum(jet_pt > 40, axis=1) >= 1\n",
        "def cut_tau_pt(tau_pt):\n",
        "    return ak.sum(tau_pt > 5, axis=1) >= 2"
      ],
      "metadata": {
        "id": "8OIgdGHwFwLw"
      },
      "id": "8OIgdGHwFwLw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Hmass(E, pt, eta, phi):\n",
        "    tau_E = ak.sum(E, axis=1)\n",
        "    tau_px = ak.sum(pt * np.cos(phi), axis=1)\n",
        "    tau_py = ak.sum(pt * np.sin(phi), axis=1)\n",
        "    tau_pz = ak.sum(pt * np.tanh(eta), axis=1)\n",
        "    Mass = np.sqrt(tau_E**2 - (tau_px**2 + tau_py**2 + tau_pz**2))\n",
        "    return Mass\n"
      ],
      "metadata": {
        "id": "ZxOs3HyrFwOL"
      },
      "id": "ZxOs3HyrFwOL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_file(file_path, tree_name, loop):\n",
        "    tree = uproot.open(f\"{file_path}:{tree_name}\")\n",
        "    sample_data = []\n",
        "\n",
        "    variables = [\"tau_pt\", \"tau_eta\", \"tau_phi\", \"tau_e\", \"tau_charge\", \"trigT\", \"lep_isTrigMatched\", \"tau_n\", \"met\", \"lep_type\", \"lep_isMediumID\", \"lep_isLooseIso\", \"jet_e\", \"jet_pt\", \"jet_eta\", \"jet_phi\", \"jet_btag_quantile\", \"ScaleFactor_TAU\", \"ScaleFactor_DiTauTRIGGER\", \"ScaleFactor_PILEUP\", \"ScaleFactor_BTAG\", \"mcWeight\", \"sum_of_weights\", \"xsec\", \"filteff\", \"kfac\"]\n",
        "\n",
        "    for data in tree.iterate(variables, library=\"ak\", entry_start=int(tree.num_entries * fraction * loop), entry_stop=int(tree.num_entries * fraction * (loop + 1))):\n",
        "        data = data[cut_tau_pt(data.tau_pt)]\n",
        "        data = data[cut_leading_jet(data.jet_pt)]\n",
        "       # data = data[cut_trig(data.trigT)]\n",
        "        data = data[cut_met_et(data.met)]\n",
        "        data['Inv_mass'] = Hmass(data.tau_e, data.tau_pt, data.tau_eta, data.tau_phi)\n",
        "        data['Weight'] = calc_weight(data)\n",
        "        sample_data.append(data)\n",
        "\n",
        "    return ak.concatenate(sample_data, axis=0)\n"
      ],
      "metadata": {
        "id": "oTNAWxi0FwUa"
      },
      "id": "oTNAWxi0FwUa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parallel_analysis(file_list, tree_name):\n",
        "    combined_data = []\n",
        "    for file_path in file_list:\n",
        "        with ProcessPoolExecutor() as executor:\n",
        "            futures = {executor.submit(process_file, file_path, tree_name, i): i for i in range(10)}\n",
        "            results = []\n",
        "            for future in as_completed(futures):\n",
        "                try:\n",
        "                    results.append(future.result())\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in file {file_path}, loop {futures[future]}: {e}\")\n",
        "        if results:\n",
        "            combined_data.append(ak.concatenate(results, axis=0))\n",
        "\n",
        "    return ak.concatenate(combined_data, axis=0) if combined_data else ak.Array([])\n"
      ],
      "metadata": {
        "id": "Ola47TvWFwW7"
      },
      "id": "Ola47TvWFwW7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main analysis loop\n",
        "start_all = time.time()\n",
        "fraction = 1\n",
        "print(\"The analysis has started\")\n",
        "\n",
        "bkg_files = [\n",
        "    files[0],  # Ztt_maxHTpTV2_Mll10_40_CVetoBVeto\n",
        "    files[1],  # ttbar_allhad\n",
        "    files[5],  # Ztautau_maxHTpTV2_BFilter\n",
        "    files[10], # Ztt_maxHTpTV2_CFilterBVeto\n",
        "    files[12]  # Ztt_maxHTpTV2_BFilter\n",
        "]\n",
        "\n",
        "signal_files = [\n",
        "    files[2],  # ggH125_tautaulm15hp20\n",
        "    files[3],  # ggH125_tautaulp15hm20\n",
        "    files[11], # ggH125_tautaul13l7\n",
        "    files[4],  # ttH125_allhad\n",
        "    files[7],  # ttH125_semilep\n",
        "]\n",
        "\n",
        "data_files = [\n",
        "   # files[6],  # data15_periodF\n",
        "   # files[8],  # data15_periodD\n",
        "    files[9],  # data15_periodH\n",
        "]\n",
        "\n",
        "bkg_data = parallel_analysis(bkg_files, tree_name)\n",
        "signal_data = parallel_analysis(signal_files, tree_name)\n",
        "data_data = parallel_analysis(data_files, tree_name)\n"
      ],
      "metadata": {
        "id": "X-HWzHQmFwZL"
      },
      "id": "X-HWzHQmFwZL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def plot_data(bkg, signal, data, fit=True):\n",
        "    xmin, xmax, step_size = 110, 160, 2\n",
        "    bin_edges = np.arange(xmin, xmax + step_size, step_size)\n",
        "    bin_centres = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
        "\n",
        "\n",
        "    data_y, _ = np.histogram(data['Inv_mass'], bins=bin_edges)\n",
        "    data_yerr = np.sqrt(data_y)\n",
        "\n",
        "\n",
        "    bkg_y, _ = np.histogram(bkg['Inv_mass'], bins=bin_edges, weights=bkg['Weight'])\n",
        "    signal_y, _ = np.histogram(signal['Inv_mass'], bins=bin_edges, weights=signal['Weight'])\n",
        "\n",
        "\n",
        "    stacked_mc = np.vstack([bkg_y, signal_y])\n",
        "    total_mc = np.sum(stacked_mc, axis=0)\n",
        "\n",
        "\n",
        "    total_mc_err = np.sqrt(\n",
        "        np.histogram(bkg['Inv_mass'], bins=bin_edges, weights=bkg['Weight']**2)[0] +\n",
        "        np.histogram(signal['Inv_mass'], bins=bin_edges, weights=signal['Weight']**2)[0]\n",
        "    )\n",
        "\n",
        "    fig, (main_ax, ratio_ax) = plt.subplots(2, 1, figsize=(7, 6), gridspec_kw={'height_ratios': [3, 1]}, sharex=True)\n",
        "\n",
        "    main_ax.errorbar(bin_centres, data_y, yerr=data_yerr, fmt='ko', label=f'Data ({sum(data_y)} entries)')\n",
        "\n",
        "\n",
        "    main_ax.hist([bkg['Inv_mass'], signal['Inv_mass']],\n",
        "                 bins=bin_edges,\n",
        "                 weights=[bkg['Weight'], signal['Weight']],\n",
        "                 stacked=True,\n",
        "                 color=['cyan', 'red'],\n",
        "                 label=[r'$Bkg$', r'$Signal$'])\n",
        "\n",
        "\n",
        "    main_ax.bar(bin_centres, 2 * total_mc_err, alpha=0.5, bottom=total_mc - total_mc_err,\n",
        "                color='none', hatch=\"////\", width=step_size, label='Stat. unc.')\n",
        "\n",
        "    main_ax.set_xlim(xmin, xmax)\n",
        "    main_ax.set_ylim(0, max(max(data_y), max(total_mc)) * 1.5)\n",
        "    main_ax.set_ylabel(\"Events\")\n",
        "    main_ax.text(0.05, 0.93, 'ATLAS Open Data', transform=main_ax.transAxes, fontsize=13)\n",
        "    main_ax.text(0.05, 0.88, 'for education', transform=main_ax.transAxes, style='italic', fontsize=8)\n",
        "    main_ax.text(0.05, 0.82, r'$\\sqrt{s}=13$ TeV, 36 fb$^{-1}$', transform=main_ax.transAxes)\n",
        "    main_ax.legend(frameon=False)\n",
        "\n",
        "\n",
        "    ratio = data_y / total_mc\n",
        "    ratio_err = ratio * np.sqrt((data_yerr / data_y)**2 + (total_mc_err / total_mc)**2)\n",
        "    ratio_ax.errorbar(bin_centres, ratio, yerr=ratio_err, fmt='ko')\n",
        "    ratio_ax.axhline(1, color='red', linestyle='--')\n",
        "    ratio_ax.set_ylabel(\"Data / MC\")\n",
        "    ratio_ax.set_ylim(0.5, 1.5)\n",
        "    ratio_ax.set_xlabel(r\"$m_{\\tau\\tau}$ [GeV]\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(hspace=0.05)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_data(bkg_data, signal_data, data_data)\n"
      ],
      "metadata": {
        "id": "wSNAK4vmQqC9"
      },
      "id": "wSNAK4vmQqC9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import AutoMinorLocator\n",
        "from scipy.optimize import curve_fit\n",
        "\n",
        "# Gaussian\n",
        "def gaussian(x, A, mu, sigma):\n",
        "    return A * np.exp(-0.5 * ((x - mu) / sigma)**2)\n",
        "\n",
        "#polynomial\n",
        "def poly2(x, a, b, c):\n",
        "    return a * x**2 + b * x + c\n",
        "\n",
        "def poly3(x, a, b, c, d):\n",
        "    return a * x**3 + b * x**2 + c * x + d\n",
        "\n",
        "def plot_data(bkg, signal, data, fit=True):\n",
        "    xmin, xmax, step_size = 110, 160, 2\n",
        "    bin_edges = np.arange(xmin, xmax + step_size, step_size)\n",
        "    bin_centres = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
        "\n",
        "    data_y, _ = np.histogram(data['Inv_mass'], bins=bin_edges)\n",
        "    data_yerr = np.sqrt(data_y)\n",
        "\n",
        "    bkg_y, _ = np.histogram(bkg['Inv_mass'], bins=bin_edges, weights=bkg['Weight'])\n",
        "    signal_y, _ = np.histogram(signal['Inv_mass'], bins=bin_edges, weights=signal['Weight'])\n",
        "\n",
        "    stacked_mc = np.vstack([bkg_y, signal_y])\n",
        "    total_mc = np.sum(stacked_mc, axis=0)\n",
        "\n",
        "    total_mc_err = np.sqrt(\n",
        "        np.histogram(bkg['Inv_mass'], bins=bin_edges, weights=bkg['Weight']**2)[0] +\n",
        "        np.histogram(signal['Inv_mass'], bins=bin_edges, weights=signal['Weight']**2)[0]\n",
        "    )\n",
        "\n",
        "    fig, (main_ax, ratio_ax) = plt.subplots(2, 1, figsize=(7, 6), gridspec_kw={'height_ratios': [3, 1]}, sharex=True)\n",
        "    main_ax.errorbar(bin_centres, data_y, yerr=data_yerr, fmt='ko', label=f'Data ({sum(data_y)} entries)')\n",
        "\n",
        "\n",
        "    main_ax.hist([bkg['Inv_mass'], signal['Inv_mass']],\n",
        "                 bins=bin_edges,\n",
        "                 weights=[bkg['Weight'], signal['Weight']],\n",
        "                 stacked=True,\n",
        "                 color=['cyan', 'red'],\n",
        "                 label=[r'$Bkg$', r'$Signal$'])\n",
        "\n",
        "   #stat error\n",
        "    main_ax.bar(bin_centres, 2 * total_mc_err, alpha=0.5, bottom=total_mc - total_mc_err,\n",
        "                color='none', hatch=\"////\", width=step_size, label='Stat. unc.')\n",
        "\n",
        "    # Fit\n",
        "    if fit:\n",
        "        mask = data_y > 0\n",
        "        try:\n",
        "            popt, pcov = curve_fit(gaussian, bin_centres[mask], data_y[mask], p0=[max(data_y), 125, 10], sigma=data_yerr[mask])\n",
        "            fit_curve = gaussian(bin_centres, *popt)\n",
        "            main_ax.plot(bin_centres, fit_curve, 'm--', linewidth=2, label=f'Gaussian Fit\\n$\\mu$={popt[1]:.1f}, $\\sigma$={popt[2]:.1f}')\n",
        "        except RuntimeError:\n",
        "            print(\"Fit failed: could not converge.\")\n",
        "\n",
        "        try:\n",
        "            popt_poly, pcov_poly = curve_fit(poly2, bin_centres[mask], data_y[mask], sigma=data_yerr[mask])\n",
        "            fit_poly_curve = poly2(bin_centres, *popt_poly)\n",
        "            main_ax.plot(bin_centres, fit_poly_curve, 'g--', linewidth=2, label=f'Poly3 Fit')\n",
        "        except RuntimeError:\n",
        "            print(\"Polynomial fit failed.\")\n",
        "\n",
        "    main_ax.set_xlim(xmin, xmax)\n",
        "    main_ax.set_ylim(0, max(max(data_y), max(total_mc)) * 1.5)\n",
        "    main_ax.set_ylabel(\"Events\")\n",
        "    main_ax.text(0.05, 0.93, 'ATLAS Open Data', transform=main_ax.transAxes, fontsize=13)\n",
        "    main_ax.text(0.05, 0.88, 'for education', transform=main_ax.transAxes, style='italic', fontsize=8)\n",
        "    main_ax.text(0.05, 0.82, r'$\\sqrt{s}=13$ TeV, 36 fb$^{-1}$', transform=main_ax.transAxes)\n",
        "    main_ax.legend(frameon=False)\n",
        "\n",
        "    ratio = data_y / total_mc\n",
        "    ratio_err = ratio * np.sqrt((data_yerr / data_y)**2 + (total_mc_err / total_mc)**2)\n",
        "    ratio_ax.errorbar(bin_centres, ratio, yerr=ratio_err, fmt='ko')\n",
        "    ratio_ax.axhline(1, color='red', linestyle='--')\n",
        "    ratio_ax.set_ylabel(\"Data / MC\")\n",
        "    ratio_ax.set_ylim(0.5, 3.5)\n",
        "    ratio_ax.set_xlabel(r\"$m_{\\tau\\tau}$ [GeV]\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(hspace=0.05)\n",
        "    plt.show()\n",
        "\n",
        "plot_data(bkg_data, signal_data, data_data)"
      ],
      "metadata": {
        "id": "J2y9c_JZWPAr"
      },
      "id": "J2y9c_JZWPAr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0es3rVr1akO7"
      },
      "id": "0es3rVr1akO7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "colab": {
      "provenance": [],
      "history_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}